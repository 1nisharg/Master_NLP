{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NLP -- Natural Language Processing"
      ],
      "metadata": {
        "id": "UaSQg1mSBqZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Procedure of NLP\n",
        "- Import General libraries,NLTK, SPACY.\n",
        "- Load the dataset\n",
        "- Text pre-processing like removing html tags, removing punctuations, removing stop words, expanding contractions.\n",
        "- Apply Tokenization\n",
        "- Apply Stemming\n",
        "- Apply POS Tagging\n",
        "- Apply lemmatization\n",
        "- Apply label encoding\n",
        "- Feature extraction\n",
        "- Text to numerical vector conversion with applying BOW(Count-vectorizer), applying TFIDF vectorizer, Word2vector and Glove.\n",
        "- Data preprocessing\n",
        "- Model building"
      ],
      "metadata": {
        "id": "GIQ2O225BvAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Preprocessing steps\n",
        "Text  preprocessing is an important step in NLP as it consists of cleaning our text data in order to convert it into a presentable format that is analyzable and predictable for our task is known as text preprocessing.\n",
        "\n",
        "###Basic techniques:\n",
        "- Lowering case\n",
        "- Remove punctuations\n",
        "- removal of special characters and numbers\n",
        "- removal of html tags\n",
        "- removal of url's\n",
        "- removal of extra spaces\n",
        "- expanding contraction\n",
        "- text correction\n",
        "\n",
        "###Advanced techniques:\n",
        "- Apply tokenization\n",
        "- stop word removal\n",
        "- apply stemming\n",
        "- apply lemmatization\n",
        "\n",
        "###More Advanced techniques:\n",
        "- POS(part-of-speech) tagging\n",
        "- NER (name entity recognation)\n"
      ],
      "metadata": {
        "id": "6IMl9I4nDFOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Lets now code this techniques and get some hands-on experience."
      ],
      "metadata": {
        "id": "fd-5i9zjEfY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Lowering case\n",
        "\n",
        "Why it is essential? because same words, one in upper case and other in lower case are considered as different words while creating Bag Of Words.\n",
        "\n",
        "In TF-IDF count vectorization techniques, the frequency of words is considered with irrespective of the case.\n",
        "\n",
        "Lowering decrease the vocabulary and hence reduce the dimensionality."
      ],
      "metadata": {
        "id": "OmiOULy3EtQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aRejvxWvAQFB",
        "outputId": "55b528ec-770f-4b3f-bb08-c679ccd548a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what are you doing? i am doing great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "Sent=\"What are you doing? I am doing great!\"\n",
        "sent_lower=str(Sent).lower()\n",
        "sent_lower"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Removing punctuations"
      ],
      "metadata": {
        "id": "PTftocD3HweR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punc=string.punctuation\n",
        "punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LgRTWCoJHgCE",
        "outputId": "3c993f60-6c6d-44e6-fea4-7be256fc71b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sent=\"What are you doing? I am doing great!\"\n",
        "sent_punc=[word for word in Sent.split(\" \") if word not in list(punc)]\n",
        "sent_punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMUMexaTH7oT",
        "outputId": "d35c88a1-f4e7-4930-d7fb-f56eac0cc006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What', 'are', 'you', 'doing?', 'I', 'am', 'doing', 'great!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Removing special characters and numbers\n",
        "\n",
        "Special Characters and numbers like \"!,@,#,%,^,&,$,+,*, 1 to 9\" have no meaning in the sentence and they do not contribute to any sentence classification.\n",
        "\n",
        "And there is one senario when these special charactersattached to any word will considered as different word which is alreadypresent in the sentence. eg. \"Shocked\" and \"Shocked!\" considered as different words but we know they have same meaning. Hence its better to remove anyspecial characters there for dimensionality is also reduces."
      ],
      "metadata": {
        "id": "e3cbDn1BIYpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentence=\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\"\n",
        "sent_clean=re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
        "sent_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "S8REYLFJIMX_",
        "outputId": "6d7eec36-f2c6-43b5-e35e-66523eb5209c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Find the remainder when  math          math  is divided by       '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Removal of HTML tags\n",
        "\n",
        "When we Scrap data from any website then dataset contains HTML tags. Wemight face problem if HTML Tags present in our dataset. Hence it prefered toremove these tags."
      ],
      "metadata": {
        "id": "pxHLemFcLRMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent='''<h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>'''\n",
        "sent_html=re.sub(\"<.*?>\", \"\", sent)\n",
        "sent_html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kXUkt6m7JYoC",
        "outputId": "1c4dd246-db7e-4188-81c1-defedb632f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Guys How Are You'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Removing URL's\n",
        "\n"
      ],
      "metadata": {
        "id": "6TCBl3PPMMnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sen=\"visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Senti\"\n",
        "clean_sent=re.sub(\"(http|https|www)\\S+\", \"\", sen)\n",
        "clean_sent"
      ],
      "metadata": {
        "id": "tN2l1PgqL2zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Removing Extra Spaces\n",
        "\n",
        "There is some senario where users insert extra spaces at the start, at the end\n",
        "or at the anywhere in the sentence. We need to remove all the extra spaces\n",
        "inserted by an user.\n"
      ],
      "metadata": {
        "id": "DkKmr93iN1_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senten=\"Hi   team   how  are    you ??\"\n",
        "cleaned_sen=re.sub(\" +\", \" \", senten)\n",
        "cleaned_sen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ce1MSFVSOAOl",
        "outputId": "bf9374d2-b90d-47ad-b149-bdb3c7905612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi team how are you ??'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Expanding contraction\n",
        "\n",
        "Contractions are words or combinations of words that are shortened by\n",
        "dropping letters and replacing them by an apostrophe. Nowadays, where\n",
        "everything is shifting online, we communicate with others more through text\n",
        "messages or posts on different social media like Facebook, Instagram,\n",
        "Whatsapp, Twitter, LinkedIn, etc. in the form of texts. With so many people to\n",
        "talk, we rely on abbreviations and shortened form of words for texting people.\n"
      ],
      "metadata": {
        "id": "_ELREGdWOWqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uFvAVAmbPYL_",
        "outputId": "f83563fb-5049-4ac9-9db8-b5882df51cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "sent=\"we have reached final step of our data science internship. We'll send offer letter soon.\"\n",
        "sent_cont=contractions.fix(sent)\n",
        "sent_cont"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ao2E8EUxOOa_",
        "outputId": "261501d9-7ab8-4bf4-93f6-73adad4e35ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we have reached final step of our data science internship. We will send offer letter soon.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFhXXeJaPLQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Text Correction\n",
        "\n",
        "To correct the text we are going to use TextBlob from NLTK."
      ],
      "metadata": {
        "id": "R8MUz7RxPd5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TextBlob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m8bCxd1BPt-S",
        "outputId": "6f28c72a-f669-425b-f0a4-f7d534b8a870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from TextBlob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->TextBlob) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "sentenced=\"We'll meet youu soon\"\n",
        "text=TextBlob(sentenced)\n",
        "correct_sen=text.correct()\n",
        "correct_sen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8L-9lYEPzNM",
        "outputId": "6993527c-d96f-453b-dc8c-876d2f80d037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"He'll meet you soon\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advanced Techniques"
      ],
      "metadata": {
        "id": "joPVqSo-qbSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Apply Tokenization\n",
        "\n",
        "- Process of breaking down sentence into words/tokens. It can be words, characters, or subwords.\n",
        "\n",
        "####Types of tokenization:\n",
        "(a) Sentence Tokenization\n",
        "(b) Word Tokenization\n",
        "(c) SubWord(n-gram characters) Tokenization\n",
        "\n",
        "\n",
        "Here we can use string \"Split\" method for word tokenization only. For Charcter\n",
        "and SubWord Tokenization we need to use \"NLTK\" in-buit function."
      ],
      "metadata": {
        "id": "Md9eU1B0qlf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E-L28NVuraMo",
        "outputId": "6279b5b2-02e0-40ab-c37f-88e176d526a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Sentence Tokenization\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent=\"I am Nisharg Nargund. Founder of OpenRAG - A GenerativeAI Startup\"\n",
        "tokens=sent_tokenize(sent)\n",
        "tokens\n",
        "\n",
        "\n",
        "#2. Word Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence=\"I am Nisharg Nargund. Founder of OpenRAG - A GenerativeAI Startup\"\n",
        "tokens=word_tokenize(sentence)  #token=sentence.split(\" \")\n",
        "tokens\n",
        "\n",
        "#3. SubWord(n-gram character) Tokenization\n",
        "\n",
        "from nltk import ngrams\n",
        "sen=\"I am Nisharg Nargund. Founder of OpenRAG - A GenerativeAI Startup\"\n",
        "n_gram=list(ngrams((sentence.split(\" \")),n=3))\n",
        "n_gram\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVwuhGVhrEO_",
        "outputId": "c0826549-16d1-4d86-ee34-d11c15d25e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'am', 'Nisharg'),\n",
              " ('am', 'Nisharg', 'Nargund.'),\n",
              " ('Nisharg', 'Nargund.', 'Founder'),\n",
              " ('Nargund.', 'Founder', 'of'),\n",
              " ('Founder', 'of', 'OpenRAG'),\n",
              " ('of', 'OpenRAG', '-'),\n",
              " ('OpenRAG', '-', 'A'),\n",
              " ('-', 'A', 'GenerativeAI'),\n",
              " ('A', 'GenerativeAI', 'Startup')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Remove stopwords"
      ],
      "metadata": {
        "id": "8HISzHrDsx7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5OLH7mFt77g",
        "outputId": "263be11b-2035-49fe-fc8d-b59332c0a8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords_eng=stopwords.words(\"english\")\n",
        "print(len(stopwords_eng)) #English language contains 179 Stop Words."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKTC_BB1rVua",
        "outputId": "cc45035f-a51d-4bb4-f9cd-c5c7c6b0cfb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"Our Team name is Team Data Dynamos and we have selected Quora question similar\"\n",
        "sentence_non_stopword=[word for word in sentence.split(\" \") if not word in stopwords_eng]\n",
        "print(\"Sentence with StopWOrds:\", sentence)\n",
        "print(\"Sentence without StopWords:\", \" \".join(sentence_non_stopword))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdbUzfRSt5T2",
        "outputId": "d7fae1b3-2563-41be-f956-461e2e7155e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence with StopWOrds: Our Team name is Team Data Dynamos and we have selected Quora question similar\n",
            "Sentence without StopWords: Our Team name Team Data Dynamos selected Quora question similar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. Apply Stemming"
      ],
      "metadata": {
        "id": "HOavcwqau0Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of Stemmer in NLP:\n",
        "\n",
        "a. Porter Stemmer\n",
        "\n",
        "b. Snowball Stemmer\n",
        "\n",
        "c. Lancaster Stemmer\n",
        "\n",
        "d. Regexp Stemmer"
      ],
      "metadata": {
        "id": "DxrK5_RwXwOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####a. Porter Stemmer\n",
        "\n",
        "- Stems illogical or non-dictionary word."
      ],
      "metadata": {
        "id": "7ki6RBusX7Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter=PorterStemmer()\n",
        "sentence=\"Hello guys! I am Nisharg Nargund\"\n",
        "porter_stem=[porter.stem(word) for word in sentence.split(\" \")]\n",
        "porter_stem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdXRMsqaujgN",
        "outputId": "464c2235-a51c-4ede-eaf0-462b9981e632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'guys!', 'i', 'am', 'nisharg', 'nargund']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####b. Snowball Stemmer\n",
        "\n",
        "- Stems faster and logical than porter stemmer."
      ],
      "metadata": {
        "id": "pjRs0cC6Yj8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball=SnowballStemmer(\"english\")\n",
        "sentence=\"Hello all! I am Nisharg Nargund\"\n",
        "snow_stem=[snowball.stem(word) for word in sentence.split(\" \")]\n",
        "snow_stem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLhjpZd9YiB4",
        "outputId": "b116480f-3ae0-444d-a432-0fb0ce75a467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'all!', 'i', 'am', 'nisharg', 'nargund']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####c. Lancaster stemmer\n",
        "\n",
        "- More aggresive and dynamic. Algorithm is a bit confusing when it comes in dealing with small words."
      ],
      "metadata": {
        "id": "MK1lvAS0ZaX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster=LancasterStemmer()\n",
        "Sent=\"Hello all! I am nisharg nargund\"\n",
        "lans_sent=[lancaster.stem(word) for word in Sent.split(\" \")]\n",
        "lans_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MldA5ToLY3kC",
        "outputId": "07b7dab9-f5d5-489e-e179-fbaefbf52b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'all!', 'i', 'am', 'nisharg', 'nargund']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####d. Regexp Stemmer\n",
        "\n",
        "- Identifies morphological affixes using regular expressions. Substrings matching the regular expressions will be discarded."
      ],
      "metadata": {
        "id": "3hJs6HLZb3_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regex=RegexpStemmer(regexp=\"ing$|s$|e$\", min=0)\n",
        "sent=\"Hello all! I am founder of OpenRAG and my teams working hard for unicorn\"\n",
        "regex_stem=[regex.stem(word) for word in sent.split(\" \")]\n",
        "regex_stem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVu3347Ra43n",
        "outputId": "3dc11e07-a19d-449a-86f5-5a789039bda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'all!',\n",
              " 'I',\n",
              " 'am',\n",
              " 'founder',\n",
              " 'of',\n",
              " 'OpenRAG',\n",
              " 'and',\n",
              " 'my',\n",
              " 'team',\n",
              " 'work',\n",
              " 'hard',\n",
              " 'for',\n",
              " 'unicorn']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Apply Lemmatization\n",
        "\n",
        "####Types of lemmatization in NLP:\n",
        "\n",
        "a. WordNet Lemmatizer\n",
        "\n",
        "b. TextBlob Lemmatizer\n"
      ],
      "metadata": {
        "id": "XULiCMCreC4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###a. WordNet Lemmatizer\n",
        "\n"
      ],
      "metadata": {
        "id": "c_tjU2UFeooN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enGc-k0MfMIF",
        "outputId": "cab4f812-c39b-4a41-b6e2-e7e9e8872e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma=WordNetLemmatizer()\n",
        "sent=\"Hello all! how you all doing Keep working harder\"\n",
        "sentence_lem=[lemma.lemmatize(word) for word in sent.split(\" \")]\n",
        "sentence_lem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu1PW0tEdB4C",
        "outputId": "827b93ae-2233-4d8a-d784-1f5d0125cba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'all!', 'how', 'you', 'all', 'doing', 'Keep', 'working', 'harder']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####b. Textblob lemmatizer"
      ],
      "metadata": {
        "id": "VXSufq4Pf2Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "vTMftOkLgmS2",
        "outputId": "82eb8805-3511-4525-f58c-67b358de762d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word\n",
        "sent_text=\"The bats are hanging on their feet in upright positions\"\n",
        "sent=TextBlob(sent_text)\n",
        "textblob_lemma=[w.lemmatize() for w in sent.words]\n",
        "textblob_lemma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoxziNEGfILs",
        "outputId": "768e68ed-c461-4a82-dcfe-07c7cd9fd884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'bat',\n",
              " 'are',\n",
              " 'hanging',\n",
              " 'on',\n",
              " 'their',\n",
              " 'foot',\n",
              " 'in',\n",
              " 'upright',\n",
              " 'position']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C. More Advanced Techniques"
      ],
      "metadata": {
        "id": "sdT9JAnVPPLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####1. POS Tagging\n",
        "\n",
        "- Adding a part of Speech tags to every word in the corpus is called POS tagging. If we want to perform POS tagging then no need to remove stopwords. Its aim is to figure out the hidden connections between words which can later boost the performance of ML Model.\n",
        "\n",
        "######It can be performed using two libraries:\n",
        "#####(1) POS Tagging using NLTK\n",
        "#####(2) POS tagging using spacy"
      ],
      "metadata": {
        "id": "RK1tA9BCPT61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSDrH79wQiar",
        "outputId": "925f5a3e-59a5-4acf-a8ca-432acd55a8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS using NLTK\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "doc=word_tokenize(\"What is the capital of India\")\n",
        "for i in range(len(doc)):\n",
        "  print(\"Word:\", pos_tag(doc)[i][0], \"POS tag:\", pos_tag(doc)[i][1])"
      ],
      "metadata": {
        "id": "fjYx333wgSbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d973420a-232e-45f3-874f-8a4bbb77cb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: What POS tag: WP\n",
            "Word: is POS tag: VBZ\n",
            "Word: the POS tag: DT\n",
            "Word: capital POS tag: NN\n",
            "Word: of POS tag: IN\n",
            "Word: India POS tag: NNP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS using Spacy\n",
        "\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"What is the capital of India\")\n",
        "for word in doc:\n",
        "  print(word.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtlmKdxnQTpg",
        "outputId": "c19e7c32-dbef-49f0-99d7-01149a26ea8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRON\n",
            "AUX\n",
            "DET\n",
            "NOUN\n",
            "ADP\n",
            "PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Name Entity Recognition(NER)\n",
        "\n",
        "- NER is NLP method that extracts info from text. NEW involves detecting and categorizing important info in text known as named entities.\n",
        "\n",
        "#####NER can be performed using two libraries:\n",
        "######(1) NER using NLTK\n",
        "######(2) NER using spacy"
      ],
      "metadata": {
        "id": "-XJZUAvARfwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_en=stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "fsviVoQtRYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoAU1H-YSov_",
        "outputId": "5c276cd5-7ce8-4a9b-df80-8d25f24cd2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kszi_RPBSuBJ",
        "outputId": "196ff232-c77b-4f6a-bf4d-0ffc772c278e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_ner=\"Openrag in india is founded by Nisharg Nargund. Started back in April, 2024\"\n",
        "words=[word for word in sent_ner.split(\" \") if word not in stopwords_en]\n",
        "tagged=nltk.pos_tag(words)\n",
        "entities=nltk.ne_chunk(tagged)\n",
        "for entity in entities:\n",
        "  print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWD7XZ5LR99x",
        "outputId": "18d7578b-32fc-4dc8-942f-1d288405b756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(GPE Openrag/NNP)\n",
            "('india', 'NN')\n",
            "('founded', 'VBD')\n",
            "(PERSON Nisharg/NNP Nargund./NNP)\n",
            "('Started', 'VBD')\n",
            "('back', 'RB')\n",
            "('April,', 'NNP')\n",
            "('2024', 'CD')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NER Using spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sent=\"Openrag is a genAI startup which builds domain and market specific chatbots\"\n",
        "doc = nlp(sent)\n",
        "for entity in doc.ents:\n",
        "  print(entity.text, entity.label_)"
      ],
      "metadata": {
        "id": "EGJheL40SmKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba777f3c-2c86-4200-dab6-5d2fc8035ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "genAI GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text to Numerical Vector Conversion\n",
        "\n",
        "####Major techniques are:\n",
        "(1) Bag of Word: Count vectorizer\n",
        "\n",
        "(2) TF-IDF (Term frequence-Inverse document frequency)\n",
        "\n",
        "(3) Word2Vec(Word to Vector)\n",
        "\n",
        "(4) GloVe(Global Vector)\n",
        "\n",
        "(5) BERT (Bidirectional Encoder representations from transformers)\n"
      ],
      "metadata": {
        "id": "576yx6gNGRpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Bag of Word(Count Vectorizer)\n",
        "\n",
        "Advantages:\n",
        "\n",
        "\n",
        "a. Simple Procedure and easy to implement.\n",
        "\n",
        "\n",
        "b. Easy to Understand\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "\n",
        "\n",
        "a. Does not consider the symmentic meaning of the word.\n",
        "\n",
        "\n",
        " b. Due to large vector size computational time is high.\n",
        "\n",
        "\n",
        " c. Count Vectorizer Generates Spars matrix.\n",
        "\n",
        "\n",
        " d. Out of Vocabulary words are not captured."
      ],
      "metadata": {
        "id": "BkzzRd3HGwIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. TF-IDF\n",
        "\n",
        "- Measures how important the term or word is within a document or sentence relative to a collection of documents or corpus. weightage for those words is given high if that word\n",
        "occuring in that document but occuring less in corpus\n"
      ],
      "metadata": {
        "id": "8ISLqZXBHKiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. Word2Vec\n",
        "\n",
        "- A pre-trained Word Embed Model. It creates vectors of the words that are distributed numerical representations of word features. These\n",
        "word features represents the context for the each words present in vocabulary.\n",
        "\n",
        "The vectors capture semantic associations between words, such as their meaning and proximity to other words. For example, the relationship between Italy and Rome is similar to the relationship between France and Paris, so Italy – Rome + Paris ≈ France.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "\n",
        "a. Word embeddings eventually help in establishing the association of a\n",
        "word with another similar meaning word through the created vectors.\n",
        "\n",
        "\n",
        "b. Captures symmantic meaning.\n",
        "\n",
        "\n",
        " c. Low Dimensional vectors hence the computational time reduces.\n",
        "\n",
        "\n",
        "d. Dense vectors.\n",
        "\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "\n",
        "a. Contexual meaning only captured within the window size. or in other\n",
        "word it has local context scope.\n",
        "\n",
        "\n",
        "b. Not able to generate vectors for unseen words"
      ],
      "metadata": {
        "id": "mi-sgxgCHp8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. GloVe(Global Vector)\n",
        "\n",
        "It is also a Pre-trained word embedding technique used to overcome drawback\n",
        "of Word2Vec.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "\n",
        "   a. Contexual meaning captured for both local and global scope.\n",
        "\n",
        "\n",
        " b. It uses co-occurance matrix to tell us how often two words occuring\n",
        "together.\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "\n",
        "a. Utilizes massive memory and takes time to load.\n"
      ],
      "metadata": {
        "id": "MzoDpiYjIuiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. BERT\n",
        "\n",
        "BERT is the Pre-trained birectional trasformer for Language understanding. It\n",
        "has trained on 2500M Wikipedia words and 800M+ Books words. And BERT\n",
        "used by Google search Engine. BERT uses the encoder part of the\n",
        "Transformer, since it’s goal is to create a model that performs a number of\n",
        "different NLP tasks.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "\n",
        " a. Contexual meaning captured for both local and global scope.\n",
        "\n",
        "\n",
        " b. Captures symmantic meaning.\n",
        "\n",
        "\n",
        " c. Powerful than all previous wod embedding techniques.\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "\n",
        " a. Utilizes massive memory and takes time to load and train.\n"
      ],
      "metadata": {
        "id": "df954i3GJAX-"
      }
    }
  ]
}